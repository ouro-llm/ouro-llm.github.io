<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Ouro: Scaling Latent Reasoning via Looped Language Models">
  <meta name="keywords" content="Ouro, Looped Language Models, LoopLM, Latent Reasoning, Parameter Efficiency, Recursive Computation, ByteDance Seed, UC Santa Cruz">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Ouro: Looped Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome_6_7_2.all.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome_6_7_2.all.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> -->

      <!-- <div class="navbar-item has-dropdown is-hoverable"> -->
        <!-- <a class="navbar-link">
          More Research
        </a> -->
        <!-- <div class="navbar-dropdown"> -->
          <!-- <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a> -->
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> -->

  <!-- </div> -->
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            Scaling Latent Reasoning via Looped Language Models
          </h1>
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Author 1</a><sup>1,2,3</sup>,</span>
            <span class="author-block">
              <a href="">Author 2</a><sup>1,2,3</sup>,</span>
            <span class="author-block">
              <a href="">Author 3</a><sup>1,2,3</sup>,
            </span>
          </div> -->

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ByteDance Seed</span>
            &nbsp;
            <span class="author-block"><sup>2</sup>UC Santa Cruz</span>
            &nbsp;
            <span class="author-block"><sup>3</sup>Princeton University</span>
            <br>
            <span class="author-block"><sup>4</sup>Mila - Quebec AI Institute</span>
            &nbsp;
            <span class="author-block"><sup>5</sup>University of Montreal</span>
            &nbsp;
            <span class="author-block"><sup>6</sup>Carnegie Mellon University</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">*Full author list in paper</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2510.25741"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-solid fa-file-pdf" style="color: #ec4646;"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Arxiv Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
            </div>
            <!-- Model Links -->
            <div class="publication-links" style="margin-top: 1rem;">
              <span class="link-block">
                <a href="https://huggingface.co/ByteDance/Ouro-1.4B"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-solid fa-face-smiling-hands" style="color: #FFD43B;"></i>
                  </span>
                  <span>Ouro-1.4B</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/ByteDance/Ouro-2.6B"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-solid fa-face-smiling-hands" style="color: #FFD43B;"></i>
                  </span>
                  <span>Ouro-2.6B</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/ByteDance/Ouro-1.4B-Thinking"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-solid fa-brain" style="color: #9B59B6;"></i>
                  </span>
                  <span>Ouro-1.4B-Thinking</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/ByteDance/Ouro-2.6B-Thinking"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-solid fa-brain" style="color: #9B59B6;"></i>
                  </span>
                  <span>Ouro-2.6B-Thinking</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- News Section -->
<section class="section" style="background-color: #f5f5f5; padding-top: 2rem; padding-bottom: 2rem;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">ðŸ“¢ News</h2>
        <div class="content">
          
          <!-- News Item 1 -->
          <div class="box" style="border-left: 4px solid #3273dc; margin-bottom: 1.5rem;">
            <div style="display: flex; align-items: baseline;">
              <span style="background-color: #3273dc; color: white; padding: 0.2em 0.6em; border-radius: 4px; font-size: 0.85em; font-weight: bold; margin-right: 1em;">
                2025-10-30
              </span>
              <p style="margin: 0;">vLLM and SGLang integration is ready! Check out the PR <a href="https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro.html">here</a>. </p>
            </div>
          </div>

          <!-- News Item 1 -->
          <div class="box" style="border-left: 4px solid #3273dc; margin-bottom: 1.5rem;">
            <div style="display: flex; align-items: baseline;">
              <span style="background-color: #3273dc; color: white; padding: 0.2em 0.6em; border-radius: 4px; font-size: 0.85em; font-weight: bold; margin-right: 1em;">
                2025-10-28
              </span>
              <p style="margin: 0;">vLLM and SGLang integration will come soon!</p>
            </div>
          </div>

          <!-- Placeholder for future news -->
          <!-- 
          <div class="box" style="border-left: 4px solid #3273dc; margin-bottom: 1.5rem;">
            <div style="display: flex; align-items: baseline; margin-bottom: 0.5rem;">
              <span style="background-color: #3273dc; color: white; padding: 0.2em 0.6em; border-radius: 4px; font-size: 0.85em; font-weight: bold; margin-right: 1em;">
                YYYY-MM-DD
              </span>
              <h4 style="margin: 0; font-size: 1.1em; font-weight: bold;">News Title</h4>
            </div>
            <p>News content goes here...</p>
          </div>
          -->

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introducing Ouro</h2>
        <div class="content has-text-justified">
          <p>
            Modern LLMs are trained to 'think' primarily via explicit text generation, such as chain-of-thought, which defers reasoning to post-training and under-leverages pre-training data. We present <b>Ouro</b>, named after the recursive Ouroboros, a family of pre-trained <b>Looped Language Models</b> that instead build reasoning into the pre-training phase through (i) <b>iterative computation in latent space</b>, (ii) an <b>entropy-regularized objective</b> for learned depth allocation, and (iii) scaling to <span style="font-weight:900;">7.7T tokens</span>. 
          </p>
          <p>
            Through controlled experiments, we show this advantage stems not from increased knowledge storage, but from <b>superior knowledge manipulation capabilities</b>. We also show our latent reasoning is more faithful to the reasoning process than standard LLMs. Our resulting <b>1.4B and 2.6B models</b> match the performance of up to <b>12B SOTA standard LLMs</b> across a wide range of benchmarks, showing its potential as a scaling direction in a data-constrained era.
          </p>
        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="subtitle has-text-centered">
          <img src="./static/images/ouro_main.png"/>
        </h2>
        Ouro Looped Language Model Architecture and Performance
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Key Features -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Key Features</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">ðŸ”„ Looped Architecture</h3>
          <p>
            Ouro uses a parameter-shared looped architecture where the same transformer blocks are applied recurrently. This allows the model to perform iterative computation in latent space, enabling deeper reasoning without proportionally increasing parameter count. Our models use 4 recurrent steps (R4) to achieve exceptional parameter efficiency.
          </p>
          <h3 class="title is-4">ðŸŽ¯ Exceptional Parameter Efficiency</h3>
          <p>
            Through 7.7T token pre-training, our models demonstrate remarkable parameter efficiency:
            <ul>
              <li><b>Ouro-1.4B</b>: Matches the performance of 4B standard transformer models</li>
              <li><b>Ouro-2.6B</b>: Rivals 8B standard transformer models</li>
              <li>Achieves 2-3Ã— parameter efficiency improvements across diverse benchmarks</li>
            </ul>
          </p>
          <h3 class="title is-4">ðŸ§  Superior Knowledge Manipulation</h3>
          <p>
            Through controlled experiments on synthetic tasks, we demonstrate that the looped architecture's advantage stems not from increased knowledge storage, but from superior knowledge manipulation capabilities on tasks requiring fact composition and multi-hop reasoning.
          </p>
          <h3 class="title is-4">ðŸ“Š Entropy-Regularized Adaptive Computation</h3>
          <p>
            We develop a novel training objective with entropy regularization that enables dynamic depth allocation. Simple inputs can exit after fewer recurrent steps, while complex problems automatically allocate more iterations, matching computational depth to input difficulty.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Training Pipeline -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Training Pipeline</h2>
        <div class="content has-text-justified">
          <p>
            Our training pipeline is a carefully designed multi-stage process totaling <b>7.7T tokens</b> of training data:
          </p>
          <ol>
            <li>
              <b>Warmup</b>: Initial model warmup phase
            </li>
            <li>
              <b>Stable Training Phase 1</b>: 3T tokens on standard pre-training data
            </li>
            <li>
              <b>Model Branching</b>: Creating 1.4B and 2.6B variants (via upcycling)
            </li>
            <li>
              <b>Stable Training Phase 2</b>: Additional 3T tokens for both model sizes
            </li>
            <li>
              <b>CT Annealing</b>: 1.4T tokens with chain-of-thought annealing
            </li>
            <li>
              <b>LongCT</b>: 20B tokens of long-context chain-of-thought training
            </li>
            <li>
              <b>Mid-Training</b>: 300B tokens of targeted mid-training
            </li>
            <li>
              <b>Reasoning SFT</b>: Supervised fine-tuning for reasoning-focused models (Ouro-Thinking variants)
            </li>
          </ol>
          <p>
            The architecture uses standard decoder-only Transformer with <b>Rotary Position Embeddings (RoPE)</b>, <b>SwiGLU activation</b>, and <b>sandwich normalization</b> for enhanced training stability with deep recurrent computation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Contributions</h2>
        <div class="content has-text-justified">
          <p>
            
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>
        <div class="content has-text-justified">
          
        </div>
      </div>
    </div>
  </div>
</section> -->


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{2025puffinzero,
                author    = {Author 1, Author 2, Author 3},
                title     = {Puffin-Zero},
                journal   = {Arxiv},
                year      = {2025},
              }</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/pdf/dapo_paper.pdf">
        <i class="fa-solid fa-file-pdf" style="color: #ec4646;"></i>
      </a> -->
      <!-- <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a> and licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
